
<!DOCTYPE html>
<!-- saved from url=(0040)http://artemsheludko.pw/flexible-jekyll/ -->
<html lang="en">

<head>
	
<!-- <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1833684faf5f254c1bb31386c5780c57";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script> -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Hanyu Zhou-周寒宇</title>
<link rel="shortcut icon" href="img/hyzhou.jpg"/>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Hanyu Zhou, Huazhong University of Science and Technology"> 
<meta name="description" content="Hanyu Zhou's home page">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<!-- <link rel="stylesheet" href="./css/jemdoc.css"> -->
<link rel="stylesheet" href="./css/jemdoc.css" type="text/css">
<title>Hanyu Zhou, Huazhong University of Science and Technology</title>
</head>

<body>
 <div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/hyzhouboy" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250"
 style="fill:#0000FF; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z">
 </path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,
 87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
 <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 
 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,
 77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,
 116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>
 <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,
 60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px)
 {.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
	
	
	
	
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Hanyu Zhou &nbsp 周寒宇 </h1><h1>
				</h1></div>

				<h3> Postdoctoral Research Fellow </h3>
				<p>
					School of Computing (SoC), <br>
					National University of Singapore (NUS), <br>
					11 Computing Drive, Singapore, 117416 <br>
					<br>
					Email:  hyzhouboy@gmail.com,  hy.zhou@nus.edu.sg
					       
				</p>
				<p>     
					
					<!-- <a href="paper/CV.pdf"><img src="img/cv_p.jpg"  height="40px" style="margin-bottom:-3px"></a> -->
					<a href="https://github.com/hyzhouboy"><img src="img/github.jpg" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://scholar.google.com/citations?user=bRXguCgAAAAJ&hl=en"><img src="img/google-logo.png"  height="40px" style="margin-bottom:-3px"></a>
					<a href="https://www.linkedin.com/in/hanyu-zhou-0b67b3274/"><img src="img/linkedin.jpg"  height="40px" style="margin-bottom:-3px"></a>
<!-- 					<a href="img/weichat.pdf"><img src="img/weichat.jpg"  height="40px" style="margin-bottom:-3px"></a> -->
				</p>
			</td>
			<td>
				<a href="https://hyzhouboy.github.io/"><img src="img/hyzhou.jpg" alt="Hanyu Zhou" border="0" height="300"></a><br>
			</td>
		</tr><tr>
	</tr>
	</tbody>
</table>
	
    <h2>Biography</h2>
    <div id="news-content" >
	  <span>
		I am currently a postdoctoral research fellow at National University of Singapore (NUS), working closely with Prof. <i><a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a></i>.

		I was a research intern at Shanghai AI Lab in 2024, working closely with <i><a href="https://scholar.google.com.hk/citations?user=DQB0hqwAAAAJ&hl=zh-CN">Bin Zhao</a></i>.
		<br><br>
		I got the Ph.D. degree at Huazhong University of Science and Technology (HUST) in 2024, advised by Prof. <i><a href="https://scholar.google.com.hk/citations?user=5CS6T8AAAAAJ&hl=en">Luxin Yan</a></i>. 
		Before that, I got the B.Eng. degree at Central South University (CSU) in 2019.
		<br><br>
		I am now working on 2D/3D/4D vision perception & understanding in adverse scenes, if you have an excellent project for collaboration, please <span style="color:Red"><strong>email me!</strong></span> </span> <br>

		<div style="display: flex; justify-content: center;">
			<img src="img/NUS_logo.jpg"  height="120px" style="margin-top: 20px; margin-left: auto; margin-right: 15px;">
			<img src="img/ailab.png"  height="100px" style="margin-top: 27px; margin-left: auto; margin-right: auto;">
			<img src="img/hust_logo.png"  height="120px" style="margin-top: 20px; margin-left: auto; margin-right: auto;">
			<img src="img/csu——logo.png"  height="120px" style="margin-top: 20px; margin-left: auto; margin-right: auto;"> 
		</div>
	</div>

	<h2>Interests</h2>
    <div id="news-content" >
	  <span>
		Embodied AI, 3D & 4D Vision, Multimodal LLM, Event Camera, Domain Adaptation. </span> <br> <br> 
    </div>
	 

      <h2>News</h2>
      <div style="height: 450px; overflow: auto;">
      <ul>
	  <li><span style="color:Red">2025.09</span>, Our diffusion-based optical flow with frame-event fusion is accepted to <strong><i>NeurIPS'25</i></strong> (<span style="color:Red"><strong>Spotlight</strong></span>), congrating to Haonan.</li>
	  <li><span style="color:Red">2025.06</span>, Our event-based LLM paper is accepted to <strong><i>ICCV'25</i></strong>.</li>
      <li><span style="color:Red">2025.06</span>, Our event-based 4D Gaussian splatting paper is accepted to <strong><i>ICCV'25</i></strong>.</li>
	  <li><span style="color:Red">2025.05</span>, I give a talk in Xi'an Jiaotong University.</li>
	  <li><span style="color:Red">2025.02</span>, Our event-based dense and continuous optical flow paper is accepted to <strong><i>CVPR'25</i></strong>.</li>
	  <li><span style="color:Red">2025.02</span>, Our event-based frame interpolation paper is accepted to <strong><i>CVPR'25</i></strong>, congrating to Haoyue.</li>
	  <li><span style="color:Red">2025.02</span>, Our event-based nighttime HDR imaging paper is accepted to <strong><i>TPAMI'25</i></strong>, congrating to Haoyue.</li>
	  <li><span style="color:Red">2024.12</span>, I join the Computer Vision and Robotic Perception (CVRP) LAB as a <strong>postdoc research fellow</strong>.</li>
	  <li><span style="color:Red">2024.10</span>, I pass my Ph.D. defense, and become a <strong>doctor of engineering</strong>.</li>
	  <li><span style="color:Red">2024.09</span>, Our adverse weather optical flow paper is accepted to <strong><i>TPAMI'24</i></strong>.</li>
	  <li><span style="color:Red">2024.06</span>, We have won <strong>1st place</strong> in the track 'Text Recognition through Atmospheric Turbulence' in the <strong><i>CVPR'24 7th UG2+ Challenge</i></strong>.</li>
	  <li><span style="color:Red">2024.06</span>, We have won <strong>1st place</strong> in the track 'Coded Target Restoration through Atmospheric Turbulence' in the <strong><i>CVPR'24 7th UG2+ Challenge</i></strong>.</li>
      <li><span style="color:Red">2024.02</span>, Our VisMoFlow scene flow based on multimodal fusion is accepted to <strong><i>CVPR'24</i></strong>.</li>
      <li><span style="color:Red">2024.02</span>, Our NER-Net nighttime event reconstruction is accepted to <strong><i>CVPR'24</i></strong>.</li>
      <li><span style="color:Red">2024.01</span>, Our JSTR event-based moving object detection method is accepted to <strong><i>ICRA'24</i></strong>.</li>
	  <li><span style="color:Red">2024.01</span>, Our ABDA-Flow optical flow under nighttime scene paper is accepted to <strong><i>ICLR'24</i></strong> (<span style="color:Red"><strong>Spotlight</strong></span>).</li>
      <li><span style="color:Red">2023.02</span>, Our UCDA-Flow optical flow under foggy scene paper is accepted to <strong><i>CVPR'23</i></strong>.</li>
	  <li><span style="color:Red">2022.11</span>, Our HMBA-FlowNet optical flow under adverse weather paper is accepted to <strong><i>AAAI'23</i></strong>.</li>
      <li><span style="color:Red">2021.01</span>, Our JRGR derain paper is accepted to <strong><i>CVPR'21</i></strong>.</li>
      </ul>
    </div>
     
     <h2>Researches</h2>
    <div id="news-content" >
	   <span>
		I like to explore how unmanned systems (<i>e.g.</i>, uav and robotic) can emulate human behavior to relieve humans from manual tasks.
		Similar to humans, unmanned systems perceive the world via vision sensors and then perform corresponding actions in response.
		I realize that unmanned systems work well in ideal environments, but fail in adverse conditions, such as adverse weather, low light, fast motion and so on. 
		Focusing on these challenging scenes, I have devoted myself to a new research topic: <strong>Multimodal-IPUP</strong>, <i>i.e.</i>, scene imaging, scene perception, scene understanding and scene planning.
		<br> <br> 

		<!-- <span style="display: block; margin-left: auto; margin-right: auto;"> -->
		<div style="display: flex; justify-content: center;">
			<img src="img/ResearchStatement.gif" height = "550" style="margin-left: auto; margin-right: auto;"></div>
		<br>
		
		1. Regarding the issue of unmanned system, I construct a RGB-Event-LiDAR-IMU multimodality hardware system for 2D/3D/4D adverse scenes, 
		and build a large-scale multimodal dataset under various time and various weather. 
		The research outputs have been published in <strong><i><a href="https://arxiv.org/pdf/2408.08500">arXiv</a></i></strong>. More will be open soon.
        <br> <br>

		

		2. Regarding the issue of scene imaging, I propose multimodal fusion methods for 2D HDR imaging, 2D frame interpolation and 3D/4D scene reconstruction.
		The research outputs have been published in 
		<strong><i><a href="https://arxiv.org/pdf/2506.23157">ICCV 2025</a></i></strong>, 
		<strong><i><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_TimeTracker_Event-based_Continuous_Point_Tracking_for_Video_Frame_Interpolation_with_CVPR_2025_paper.pdf">CVPR 2025</a></i></strong>, 
		<strong><i><a href="https://ieeexplore.ieee.org/abstract/document/10904298">TPAMI 2025</a></i></strong>, 
		<strong><i><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Seeing_Motion_at_Nighttime_with_an_Event_Camera_CVPR_2024_paper.pdf">CVPR 2024</a></i></strong>. 
		<br> <br>

		3. Regarding the issue of scene perception, I propose multimodal adaptation frameworks for 2D/3D motion flow and detection. 
		The research outputs have been published in 
		<strong><i><a href="https://hyzhouboy.github.io/">NeurIPS 2025</a></i></strong>, 
		<strong><i><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Bridge_Frame_and_Event_Common_Spatiotemporal_Fusion_for_High-Dynamic_Scene_CVPR_2025_paper.pdf">CVPR 2025</a></i></strong>,
		<strong><i><a href="https://ieeexplore.ieee.org/document/10689387">TPAMI 2024</a></i></strong>, 
		<strong><i><a href="https://openreview.net/pdf?id=776lhoaulC">ICLR 2024</a></i></strong>,
		<strong><i><a href="https://ieeexplore.ieee.org/document/10610608">ICRA 2024</a></i></strong>,
		<strong><i><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Unsupervised_Cumulative_Domain_Adaptation_for_Foggy_Scene_Optical_Flow_CVPR_2023_paper.pdf">CVPR 2023</a></i></strong>,
		<strong><i><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25490">AAAI 2023</a></i></strong>.
        <br> <br>

		4. Regarding the issue of scene understanding, I propose large multimodal vision-language models for 2D/3D/4D reasoning, VQA, dense caption and grounding.
		The research outputs have been published in 
		<strong><i><a href="https://arxiv.org/pdf/2505.12253">arXiv</a></i></strong>,
		<strong><i><a href="https://arxiv.org/pdf/2503.06934">ICCV 2025</a></i></strong>. 
		More will be coming soon.
		<br> <br>

		5. Regarding the issue of scene planning, I will work on several action planning of robots based on multimodal vision.
		
		<br>
        
	</span>
	<br>
    </div>
	  
     
<tr><tr><tr><tr>
<div style="margin-top: 10px"></div>
    <h2>Publications <a href="https://scholar.google.com/citations?user=bRXguCgAAAAJ&hl=en">(Google Scholar)</a> </h2>
      
<table id="tbPublications" width="100%">
<tbody>
<span>(#: Co-First Author; *: Corresponding Author) </span>
<!-- <h3 style="color: red">Conference Papers </h3>	 -->
	<tr>
    
	<td width="206">
		<img src="publication/nips2025-difflow.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://hyzhouboy.github.io/" target="_blank"> <strong>Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes</strong></a>
			<p><a href="https://haonan-wang-aurora.github.io/">Haonan Wang</a>,
				<strong>Hanyu Zhou*</strong>,
				<a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>,
				<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
		<p style="margin-top: -11px"><i>Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), <strong><span style="color:Red">Spotlight</span></strong>, 2025</i></p>
		<!-- <p> [<a href="https://arxiv.org/pdf/2503.06934">arXiv</a>] -->
			<!-- [<a href="https://github.com/hyzhouboy/LLaFEA">Code</a>] -->
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>

    <td width="206">
		<img src="publication/arxiv2025_LLaFEA.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://arxiv.org/pdf/2503.06934" target="_blank"> <strong>LLaFEA: Frame-Event Complementary Fusion for Fine-Grained Spatiotemporal Understanding in LMMs</strong></a>
			<p ><strong>Hanyu Zhou</strong>, <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a></p>
		<p style="margin-top: -11px"><i>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2025</i></p>
		<p> [<a href="https://arxiv.org/pdf/2503.06934">arXiv</a>]
			[<a href="https://github.com/hyzhouboy/LLaFEA">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>


	<td width="206">
		<img src="publication/ICCV2025-STDGS.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://arxiv.org/pdf/2506.23157" target="_blank"> <strong>STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene</strong></a>
			<p ><strong>Hanyu Zhou</strong>, 
				<a href="https://haonan-wang-aurora.github.io/">Haonan Wang</a>, 
				<a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
				<a href="https://scholar.google.com/citations?user=Hn5oJJsAAAAJ&hl=zh-CN">Yuxing Duan</a>, 
				<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a>, 
				<a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a></p>
		<p style="margin-top: -11px"><i>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2025</i></p>
		<p> [<a href="https://arxiv.org/pdf/2506.23157">arXiv</a>]
			[<a href="https://github.com/Haonan-Wang-aurora/STD-GS">Code</a>]
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<td width="206">
		<img src="publication/CVPR2025-COMST.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Bridge_Frame_and_Event_Common_Spatiotemporal_Fusion_for_High-Dynamic_Scene_CVPR_2025_paper.pdf" target="_blank"> <strong>Bridge Frame and Event: Common Spatiotemporal Fusion for High-Dynamic Scene Optical Flow</strong></a>
			<p ><strong>Hanyu Zhou</strong>, 
				<a href="https://haonan-wang-aurora.github.io/">Haonan Wang</a>, 
				<a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
				<a href="https://scholar.google.com/citations?user=Hn5oJJsAAAAJ&hl=zh-CN">Yuxing Duan</a>, 
				<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
				<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
		<p style="margin-top: -11px"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2025</i></p>
		<p> [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Bridge_Frame_and_Event_Common_Spatiotemporal_Fusion_for_High-Dynamic_Scene_CVPR_2025_paper.pdf">PDF</a>]
		    [<a href="https://arxiv.org/pdf/2503.06992">arXiv</a>]
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<td width="206">
		<img src="publication/CVPR2025-TimeTracker.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_TimeTracker_Event-based_Continuous_Point_Tracking_for_Video_Frame_Interpolation_with_CVPR_2025_paper.pdf" target="_blank"> <strong>TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion</strong></a>
			<p ><a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
				<a href="https://hyzhouboy.github.io/">Jinghan Xu</a>, 
				<a href="https://owuchangyuo.github.io/">Yi Chang</a>, <strong>Hanyu Zhou</strong>, 
				<a href="https://hyzhouboy.github.io/">Haozhi Zhao</a>, 
				<a href="https://scholar.google.com/citations?user=SReb2csAAAAJ&hl=en">Lin Wang</a>, 
				<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
		<p style="margin-top: -11px"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2025</i></p>
		 <p>[<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_TimeTracker_Event-based_Continuous_Point_Tracking_for_Video_Frame_Interpolation_with_CVPR_2025_paper.pdf">PDF</a>] 
			[<a href="https://arxiv.org/pdf/2505.03116">arXiv</a>]
		</p> 
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<td width="206">
		<img src="publication/TPAMI2025-Imaging.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://ieeexplore.ieee.org/abstract/document/10904298" target="_blank"> <strong>NER-Net+: Seeing motion at nighttime with an event camera</strong></a>
			<p ><a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
				<a href="https://hyzhouboy.github.io/">Jinghan Xu</a>, 
				<a href="https://scholar.google.com/citations?user=Xd6FVrcAAAAJ&hl=en">Shihan Peng</a>, 
				<a href="https://owuchangyuo.github.io/">Yi Chang</a>, <strong>Hanyu Zhou</strong>, 
				<a href="https://scholar.google.com/citations?user=Hn5oJJsAAAAJ&hl=zh-CN">Yuxing Duan</a>, 
				<a href="https://linzhu.tech/">Lin Zhu</a>, 
				<a href="https://scholar.google.com/citations?user=fn6hJx0AAAAJ&hl=zh-CN">Yonghong Tian</a>, 
				<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
		<p style="margin-top: -11px"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>), 2025</i></p>
		<p> [<a href="https://ieeexplore.ieee.org/abstract/document/10904298">PDF</a>]
			[<a href="https://github.com/Liu-haoyue/NER-Net">Code</a>]
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<td width="206">
		<img src="publication/TPAMI2024-H2H.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://ieeexplore.ieee.org/document/10689387" target="_blank"> <strong>Adverse Weather Optical Flow: Cumulative Homogeneous-Heterogeneous Adaptation</strong></a>
			<p ><strong>Hanyu Zhou</strong>, 
				<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
				<a href="https://hyzhouboy.github.io/">Zhiwei Shi</a>, 
				<a href="https://scholar.google.com/citations?user=VoFRbrQAAAAJ&hl=en">Wending Yan</a>, 
				<a href="https://scholar.google.com/citations?user=7GwIDigAAAAJ&hl=en">Gang Chen</a>, 
				<a href="https://scholar.google.com/citations?user=fn6hJx0AAAAJ&hl=zh-CN">Yonghong Tian</a>, 
				<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
		<p style="margin-top: -11px"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>), 2024</i></p>
		<p>[<a href="https://ieeexplore.ieee.org/document/10689387">PDF</a>] 
			[<a href="https://arxiv.org/pdf/2409.17001">arXiv</a>]
			[<a href="https://github.com/hyzhouboy/CH2DA-Flow">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>


	<td width="206">
		<img src="publication/ICLR2024-ABDA.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openreview.net/pdf?id=776lhoaulC" target="_blank"> <strong>Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow</strong></a>
		 <p ><strong>Hanyu Zhou</strong>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
			<a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
			<a href="https://scholar.google.com/citations?user=VoFRbrQAAAAJ&hl=en">Wending Yan</a>, 
			<a href="https://scholar.google.com/citations?user=Hn5oJJsAAAAJ&hl=zh-CN">Yuxing Duan</a>, 
			<a href="https://hyzhouboy.github.io/">Zhiwei Shi</a>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
       <p style="margin-top: -11px"><i>International Conference on Learning Representations (<strong> ICLR </strong>), <strong><span style="color:Red">Spotlight</span></strong>, 2024</i></p>
		<p>[<a href="https://openreview.net/pdf?id=776lhoaulC">PDF</a>] 
			[<a href="https://arxiv.org/pdf/2401.17642.pdf">arXiv</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>
	


	<tr>
		<td width="206">
		<img src="publication/CVPR2024-VisMoFlow.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Bring_Event_into_RGB_and_LiDAR_Hierarchical_Visual-Motion_Fusion_for_CVPR_2024_paper.pdf" target="_blank"> <strong>Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow</strong></a>
		 <p ><strong>Hanyu Zhou</strong>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
			<a href="https://hyzhouboy.github.io/">Zhiwei Shi</a>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
       <p style="margin-top: -11px"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024</i></p>
		<p>[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Bring_Event_into_RGB_and_LiDAR_Hierarchical_Visual-Motion_Fusion_for_CVPR_2024_paper.pdf">PDF</a>]
			[<a href="https://arxiv.org/pdf/2403.07432.pdf">arXiv</a>]</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<tr>
		<td width="206">
		<img src="publication/CVPR2024-NER-Net.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Seeing_Motion_at_Nighttime_with_an_Event_Camera_CVPR_2024_paper.pdf" target="_blank"> <strong>Seeing Motion During Nighttime with Event Camera</strong></a>
		 <p ><a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
			<a href="https://scholar.google.com/citations?user=Xd6FVrcAAAAJ&hl=en">Shihan Peng</a>, 
			<a href="https://linzhu.tech/">Lin Zhu</a>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, <strong>Hanyu Zhou</strong>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
       <p style="margin-top: -11px"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024</i></p>
		<p>[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Seeing_Motion_at_Nighttime_with_an_Event_Camera_CVPR_2024_paper.pdf">PDF</a>]
			[<a href="https://arxiv.org/pdf/2404.11884.pdf">arXiv</a>]
			[<a href="https://github.com/Liu-haoyue/NER-Net">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>


	<tr>
		<td width="206">
		<img src="publication/ICRA2024-JSTR.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://ieeexplore.ieee.org/document/10610608" target="_blank"> <strong>JSTR: Joint Spatio-Temporal Reasoning for Event-Based Moving Object Detection</strong></a>
		 <p ><strong>Hanyu Zhou</strong>, 
			<a href="https://hyzhouboy.github.io/">Zhiwei Shi</a>, 
			<a href="https://hyzhouboy.github.io/">Hao Dong</a>, 
			<a href="https://scholar.google.com/citations?user=Xd6FVrcAAAAJ&hl=en">Shihan Peng</a>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
       <p style="margin-top: -11px"><i>IEEE International Conference on Robotics and Automation (<strong> ICRA </strong>), 2024</i></p>
		<p>[<a href="https://ieeexplore.ieee.org/document/10610608">PDF</a>]
			[<a href="https://arxiv.org/pdf/2403.07436.pdf">arXiv</a>]
			[<a href="https://github.com/Alex-code-hust/IMU_based_motion_segmentation">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<tr>
		<td width="206">
		<img src="publication/CVPR2023-UCDA.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Unsupervised_Cumulative_Domain_Adaptation_for_Foggy_Scene_Optical_Flow_CVPR_2023_paper.pdf" target="_blank"> <strong>Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow</strong></a>
		 <p ><strong>Hanyu Zhou</strong>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
			<a href="https://scholar.google.com/citations?user=VoFRbrQAAAAJ&hl=en">Wending Yan</a>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
       <p style="margin-top: -11px"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2023</i></p>
		<p>[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Unsupervised_Cumulative_Domain_Adaptation_for_Foggy_Scene_Optical_Flow_CVPR_2023_paper.pdf">PDF</a>]
			[<a href="https://arxiv.org/pdf/2303.07564.pdf">arXiv</a>]
			[<a href="https://github.com/hyzhouboy/UCDA-Flow">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<tr>
		<td width="206">
		<img src="publication/AAAI2023-HMBA.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25490" target="_blank"> <strong>Unsupervised Hierarchical Domain Adaptation for Adverse Weather Optical Flow</strong></a>
		 <p ><strong>Hanyu Zhou</strong>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
			<a href="https://scholar.google.com/citations?user=7GwIDigAAAAJ&hl=en">Gang Chen</a>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
       <p style="margin-top: -11px"><i>AAAI Conference on Artificial Intelligence (<strong> AAAI </strong>), 2023</i></p>
		<p>[<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25490">PDF</a>]
			[<a href="https://arxiv.org/pdf/2303.13761.pdf">arXiv</a>]
			[<a href="https://github.com/hyzhouboy/HMBA-FlowNet">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>


	<tr>
		<td width="206">
		<img src="publication/CVPR2021-JRGR.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ye_Closing_the_Loop_Joint_Rain_Generation_and_Removal_via_Disentangled_CVPR_2021_paper.pdf" target="_blank"> <strong>Closing the Loop: Joint Rain Generation and Removal via Disentangled Image Translation</strong></a>
		 <p ><a href="https://scholar.google.com/citations?user=BqsrOoQAAAAJ&hl=en">Yuntong Ye</a>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, <strong>Hanyu Zhou</strong>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
       <p style="margin-top: -11px"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2021</i></p>
		<p>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ye_Closing_the_Loop_Joint_Rain_Generation_and_Removal_via_Disentangled_CVPR_2021_paper.pdf">PDF</a>]
			[<a href="https://arxiv.org/pdf/2103.13660.pdf">arXiv</a>]
			[<a href="https://github.com/guyii54/JRGR">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>
</tbody>
</table>

<table id="tbPublications" width="110%">
<tbody>
	<h3 style="color: red">Preprint Papers </h3>	
	
	<td width="206">
		<img src="publication/TPAMI2025-NighttimeFlow.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://hyzhouboy.github.io/" target="_blank"> <strong>Nighttime Scene Optical Flow: Common Spatio-Temporal Motion Adaptation</strong></a>
			<p ><strong>Hanyu Zhou</strong>, 
				<a href="https://haonan-wang-aurora.github.io/">Haonan Wang</a>, 
				<a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
				<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a>,
				<a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
			</p>
		<p style="margin-top: -11px"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong> TPAMI </strong>), 2025 (Under review)</i></p>
		<!-- <p>[<a href="https://ieeexplore.ieee.org/document/10689387">PDF</a>] 
			[<a href="https://arxiv.org/pdf/2409.17001">arXiv</a>] -->
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<td width="206">
		<img src="publication/Neurips-llava4d.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://arxiv.org/pdf/2505.12253" target="_blank"> <strong>LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding</strong></a>
			<p ><strong>Hanyu Zhou</strong>, <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a></p>
		<p style="margin-top: -11px"><i>arXiv, 2025</i></p>
		<p> [<a href="https://arxiv.org/pdf/2505.12253">arXiv</a>]
			[<a href="https://github.com/hyzhouboy/LLaVA-4D">Code</a>]
		</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>



	<tr>
	<td width="206">
		<img src="publication/RAL2024-CoSEC.png" width="185px" height = "100" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="https://arxiv.org/pdf/2408.08500" target="_blank"> <strong>CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving</strong></a>
		 <p ><a href="https://scholar.google.com/citations?user=Xd6FVrcAAAAJ&hl=en">Shihan Peng#</a>, <strong>Hanyu Zhou#</strong>, 
			<a href="https://hyzhouboy.github.io/">Hao Dong</a>, 
			<a href="https://hyzhouboy.github.io/">Zhiwei Shi</a>, 
			<a href="https://scholar.google.com/citations?user=DadbHdAAAAAJ&hl=zh-CN">Haoyue Liu</a>, 
			<a href="https://scholar.google.com/citations?user=Hn5oJJsAAAAJ&hl=zh-CN">Yuxing Duan</a>, 
			<a href="https://owuchangyuo.github.io/">Yi Chang</a>, 
			<a href="https://scholar.google.com/citations?user=5CS6T8AAAAAJ&hl=zh-CN">Luxin Yan</a></p>
		 <p style="margin-top: -11px"><i>arXiv, 2024</i></p>
       <!-- <p style="margin-top: -11px"><i>arXiv.</i></p> -->
		 <p>[<a href="https://arxiv.org/pdf/2408.08500">arXiv</a>]</p>
		</td>
	</tr>
	<tr></tr>
	<tr></tr>

</tbody>
</table>
	  
	 


	

    <h2><strong>Awards</strong></h2>
	<br>
   <div id="news-content">
	<li style="margin: -10px 0px 0px 5px" ><span style="color:Red">2024.06</span>, <strong> 1st Place of "Atmospheric Turbulence Mitigation" in CVPR 2024 7th UG2+ Challenge</strong>.</li>
	<li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.11</span>, <strong> Third Prize of "Robust Depth Estimation" in International Algorithm Case Competition</strong>.</li>
	<li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2019.09-2023.09</span>, <strong>Ph.D. Scholarships</strong>.</li>
	<li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2018.04</span>, <strong>Meritorious Winner in MCM/ICM</strong>.</li>
    </div>  
   
	
	 
     <h2><strong>Academic Services</strong></h2>
   <div> 
	<strong>Journal Reviewers:</strong> 
	TPAMI, TRO, IJCV, TIP, TCSVT, TMM, TIM, Sensors, MTAP, Mathematics.<br>
	
	<strong>Conference Reviewers:</strong> 
	NeurIPS'25, CVPR'23-26, ICLR'25, ICML'25, ICCV'25, ACMMM'25, WACV'26, ECCV'24, AAAI'23-26, ICRA'24-25.
    </div>
	





	<h2>Invited Talks</h2>
     <div id="news-content">
      <ul>
	<li>
		<div style="float:left; text-align:left"><strong>School of Mathematics and Statistics, Xi'an Jiaotong University</strong> (Online)</div> <div style="float:right; text-align:right">May 2025</div><br>
		<strong>Title</strong>: Multimodal-Based Imaging, Perception and Understanding in Adverse Scenes<br>
	</li>
	
	
	
	</ul>
        </div>


	   

     <div id="footer">
	<div id="footer-text"></div>
    </div>
    <div id = "logo" style="margin-top: 10px; text-align:center">
	    <div align="center" style="margin:auto;padding-top:10px">
            <div align="center">
                <!-- <script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=400&t=tt&d=RvNs3EgaHTaj7HSSlDqnISVThDMgV3AdypyqNFyt5XA'></script> -->
				<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=tt&d=_g7EuEi2n2P13A9qVnjZlOQ8ZAWxW-5o8CWk07N90ms'></script>
			</div>
           <br>
        &copy; Hanyu Zhou | <span style="color:Red">Last updated: July, 2025</span>
            </div>
     </div>     
	
    
  </div>

</body></html>
